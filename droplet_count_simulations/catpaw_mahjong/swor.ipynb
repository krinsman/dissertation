{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b52838ba-a026-4462-acd0-3b7685d5f946",
   "metadata": {},
   "source": [
    "## CTPMHg Simulation - Iterate over marginals, then over droplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68fc5211-476c-459d-a214-201f1397c0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "population_size = int(5e8)\n",
    "rate = 2\n",
    "seed = 42\n",
    "number_samples = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba10b7c3-3e4e-4e30-9642-29779fe4af49",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_relative_abundances = [1e-4, 1e-3, 1e-2]\n",
    "\n",
    "relative_abundances = [relative_abundance * number\n",
    "                       for relative_abundance \n",
    "                       in base_relative_abundances\n",
    "                       for number in (1,2,5) \n",
    "                       for repeat in range(10)]\n",
    "\n",
    "relative_abundances += [1-sum(relative_abundances)]\n",
    "frequencies = np.array(relative_abundances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "952f5ac2-6970-46eb-9274-8cd85a4e775a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTPMHg_simulation_strains_droplets(population_size, rate, seed, number_samples, frequencies):\n",
    "    # probably doing a little bit too much implicit rounding here in general but... too lazy to change\n",
    "    sub_population_sizes = (population_size * frequencies).astype(int)\n",
    "\n",
    "    remaining_population_sizes = population_size * np.ones(number_samples).astype(int)\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    remaining_sample_sizes = rng.poisson(lam=rate, size=number_samples)\n",
    "\n",
    "    cumulative_sample_sizes = np.cumsum(remaining_sample_sizes)\n",
    "    try:\n",
    "        assert cumulative_sample_sizes[-1] <= population_size\n",
    "    except AssertionError as e:\n",
    "        raise NotImplementedError(e)\n",
    "\n",
    "    remaining_population_sizes[1:] -= cumulative_sample_sizes[:-1]\n",
    "\n",
    "    remaining_sub_population_sizes = np.zeros((len(frequencies), number_samples)).astype(int)\n",
    "    remaining_sub_population_sizes[:,0] = sub_population_sizes\n",
    "\n",
    "    sample_sizes = np.zeros((len(frequencies), number_samples)).astype(int)\n",
    "\n",
    "    pop_sizes_backup = remaining_population_sizes.copy()\n",
    "    sample_sizes_backup = remaining_sample_sizes.copy()\n",
    "\n",
    "    for strain in range(len(frequencies)-1):\n",
    "        for i in range(number_samples-1):\n",
    "            strain_i_sample = rng.hypergeometric(ngood=remaining_sub_population_sizes[strain, i],\n",
    "                                                 nbad=remaining_population_sizes[i],\n",
    "                                                 nsample=remaining_sample_sizes[i])\n",
    "\n",
    "            remaining_sub_population_sizes[strain,i+1] = remaining_sub_population_sizes[strain,i] - strain_i_sample\n",
    "            sample_sizes[strain,i] = strain_i_sample\n",
    "        strain_i_sample = rng.hypergeometric(ngood=remaining_sub_population_sizes[strain, number_samples-1],\n",
    "                                            nbad=remaining_population_sizes[number_samples-1],\n",
    "                                            nsample=remaining_sample_sizes[number_samples-1])\n",
    "        sample_sizes[strain,number_samples-1] = strain_i_sample\n",
    "\n",
    "        remaining_population_sizes -= remaining_sub_population_sizes[strain,:]\n",
    "        remaining_sample_sizes -= sample_sizes[strain,:]\n",
    "\n",
    "    remaining_sub_population_sizes[len(frequencies)-1,:] = remaining_population_sizes\n",
    "    sample_sizes[len(frequencies)-1,:] = remaining_sample_sizes\n",
    "\n",
    "    assert np.all(pop_sizes_backup == np.sum(remaining_sub_population_sizes, axis=0))\n",
    "    assert np.all(sample_sizes_backup == np.sum(sample_sizes,axis=0))\n",
    "\n",
    "    return {\"pop_sizes\": remaining_sub_population_sizes, \"sample_sizes\": sample_sizes}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c8562-6fbe-4dd4-ba8d-4abb0728b43d",
   "metadata": {},
   "source": [
    "OK so basically 2 seconds for every $1,000$ droplets. Using only one core (seemingly).\n",
    "\n",
    "So $30,000$ seconds for $15$ droplets. That is under $10$ hours. It also nevertheless seems slower than what I recall for the old implementation (although that did have far fewer strains so it might not be a fair comparison)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f72037e-cd04-4f82-9bf3-167810a15395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.59 s, sys: 5.14 ms, total: 1.59 s\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = CTPMHg_simulation_strains_droplets(population_size=population_size, \n",
    "                                            rate=rate, seed=seed, \n",
    "                                            number_samples=number_samples, \n",
    "                                            frequencies=frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c887ad-e7b3-4135-b414-804ffdde0ce0",
   "metadata": {},
   "source": [
    "**Note:** returning in `dict` format is good pretty much for being lazy and not having to type as much when inputting results as arguments for `np.savez_compressed` (check out that `**` operator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6388a39a-217d-4206-bdd4-479d6835fee2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#np.savez_compressed('test.npz', **results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f959b0e-b76c-4fc0-8657-1d094d4934b9",
   "metadata": {},
   "source": [
    "## CTPMHg Simulation - Iterate over droplets, then over marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6df89d2-fd29-42b9-8b0b-8676b6d1d6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CTPMHg_simulation_droplets_strains(population_size, rate, seed, number_samples, frequencies):\n",
    "    # probably doing a little bit too much implicit rounding here in general but... too lazy to change\n",
    "    sub_population_sizes = (population_size * frequencies).astype(int)\n",
    "    \n",
    "    rng = np.random.default_rng(seed)\n",
    "    total_sample_sizes = rng.poisson(lam=rate, size=number_samples)\n",
    "\n",
    "    # seems like this variable is also only used for unit testing in this function\n",
    "    # although this unit test is more important b/c if it fails then sample wasn't\n",
    "    # actually from the truncated Poisson distribution so...\n",
    "    cumulative_sample_sizes = np.cumsum(total_sample_sizes)\n",
    "    try:\n",
    "        assert cumulative_sample_sizes[-1] <= population_size\n",
    "    except AssertionError as e:\n",
    "        raise NotImplementedError(e)\n",
    "\n",
    "    # seems like in this function I don't actually need this variable for algorithm\n",
    "    # just for like unit testing at the end of the function, that is what it seems to me\n",
    "    remaining_population_sizes = np.sum(sub_population_sizes) * np.ones(number_samples).astype(int)\n",
    "    remaining_population_sizes[1:] -= cumulative_sample_sizes[:-1]\n",
    "\n",
    "    remaining_sub_population_sizes = np.zeros((len(frequencies), number_samples)).astype(int)\n",
    "    remaining_sub_population_sizes[:,0] = sub_population_sizes\n",
    "\n",
    "    sample_sizes = np.zeros((len(frequencies), number_samples)).astype(int)\n",
    "\n",
    "    for d in range(number_samples-1):\n",
    "        droplet_d_sample = rng.multivariate_hypergeometric(\n",
    "                                            colors=remaining_sub_population_sizes[:,d],\n",
    "                                            nsample=total_sample_sizes[d],\n",
    "                                            method='marginals'\n",
    "                                            )\n",
    "        remaining_sub_population_sizes[:,d+1] = remaining_sub_population_sizes[:,d] - droplet_d_sample\n",
    "        sample_sizes[:,d] = droplet_d_sample\n",
    "        \n",
    "    droplet_d_sample = rng.multivariate_hypergeometric(\n",
    "                                        colors=remaining_sub_population_sizes[:,number_samples-1],\n",
    "                                        nsample=total_sample_sizes[number_samples-1],\n",
    "                                        method='marginals'\n",
    "                                        )\n",
    "    sample_sizes[:,number_samples-1] = droplet_d_sample\n",
    "\n",
    "    assert np.all(remaining_population_sizes == np.sum(remaining_sub_population_sizes, axis=0))\n",
    "    assert np.all(total_sample_sizes == np.sum(sample_sizes,axis=0))\n",
    "\n",
    "    return {\"pop_sizes\": remaining_sub_population_sizes, \"sample_sizes\": sample_sizes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0c9096-8f8d-46eb-9e43-ed430f457441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 88.5 ms, sys: 4.12 ms, total: 92.6 ms\n",
      "Wall time: 90.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results2 = CTPMHg_simulation_droplets_strains(population_size=population_size, \n",
    "                                            rate=rate, seed=seed, \n",
    "                                            number_samples=number_samples, \n",
    "                                            frequencies=frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af490f97-aead-469d-8d86-a3c86de58b95",
   "metadata": {},
   "source": [
    "OK well anyway the numpy iteration through marginals is _clearly_ much faster than mine...\n",
    "\n",
    "So say $0.1$ seconds per $1000$ droplets, \n",
    "\n",
    "so take previous $30,000$ seconds estimate and divide by $20$ to get $1,500$ seconds for $15$ million droplets. Which is less than an hour actually wow, even faster than what I had last time. (OK to be fair it might only be $17-18\\times$ faster, not $20$. but like that's still $1,764$ seconds only in most conservative case.)\n",
    "\n",
    "(Of course I'm not bothering to try to compute fudge factors yet, which I'm sure would/will greatly enhance computational expense. But the point is to pre-compute all of this stuff first, and then try to be able to take advantage of vectorization when computing the fudge factors, which I did not do last time.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf50348e-fefa-4106-9e44-0ab2b9cf5508",
   "metadata": {},
   "source": [
    "Also I checked and on realistic problem sizes the assertion statements and extra variables make $0$ difference on the runtime -- they are not the bottleneck (the for loop is obviously) so getting rid of them does nothing besides make the function less safe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dcc84a-842b-4a3e-b01f-0506d8487329",
   "metadata": {},
   "source": [
    "Also also -- neither this nor the above function can be turned into list comprehensions in any way (even using fancy `zip` tricks and stuff like that) because any such list would need to refer to itself (its previous entries) which apparently you can't do with a list comprehension.\n",
    "\n",
    "_Or can I?_ https://stackoverflow.com/a/51350331/10634604 OK even if I could the code would become most likely an incomprehensible mess, since I barely know anything about that syntax and I would be incrementing/updating two variables at the same time. Also this seems like one of those instances where a list comprehension would actually be slower than the for loop anyway https://stackoverflow.com/a/22108640/10634604 all the more so since I would have to do a bunch of magic to then convert the list into a numpy array afterwards so... trying to over-optimize the second function that already works would not make much sense\n",
    "\n",
    "yes I feel bad for the  first function that it can't be hyper-optimized either but oh well"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
