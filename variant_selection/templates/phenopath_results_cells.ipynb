{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(\"phenopath\")\n",
    "library(\"reticulate\")\n",
    "np <- import(\"numpy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-jenny",
   "metadata": {},
   "source": [
    "I am not sure whether the CAVI optimization from phenopath is completely deterministic, so I'm not sure whether the following cell would have actually been necessary for complete reproducibility. Nevertheless, I should have included this in the original notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-pixel",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenopath_defaults <- function(observations, covariates, ...) {\n",
    "    # suppressWarnings because I am well aware that it hasn't converged\n",
    "    return(suppressWarnings(phenopath(observations, covariates, model_mu=TRUE, \n",
    "                                      maxiter=50, thin=10, verbose=FALSE, ...)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-shelter",
   "metadata": {},
   "source": [
    "The gLV models have intercepts $\\mu$, and it makes more sense conceptually that a model has them (so they can represent 'intrinsic growth rates'), so `model_mu` is being used even though it is not a default setting. Plus very preliminary investigation seemed to suggest it is likely to get better results, and one would expect that theoretically as well not just for the reasons above but also because the default `model_mu=FALSE` is actually a special case where $\\mu = 0$ is assumed, so allowing extra possible values for that parameter in general can only increase the fit. (Of course one can then argue about overfitting, but that seems unlikely since this model is clearly heavily mis-specified anyway, and again the extra parameter would not only increase fit but also be amenable to biological interpretation in this context.)\n",
    "\n",
    "Regarding small number of iterations, they seem to give decent enough results in practice, all the more so given that in this context one is more interested in qualitative fit, especially correct signs and sparsity, than precise numerical values. In other words any resulting quantitative accuracy will hopefully not also correspond to substantially decreased qualitative accuracy, especially given that 50 iterations is still a decent amount. Plus from a practical perspective, in order to provide estimates for so many estimations, there needs to be a reasonably enough upper bound on the runtime for each, and that is only possible by providing a relatively small of maximum possible iterations. In practice the experimenter, who only needs to analyze the results of a single experiment, should probably not used such a low number of maximum iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "spearman <- function(x,y){return(cor(x,y,method='spearman'))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-anatomy",
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 11\n",
    "seed = 42\n",
    "number_droplets = 100000\n",
    "number_batches = 5\n",
    "results_dirname = 'results'\n",
    "\n",
    "base_filename = paste(results_dirname, '/', size, '_strains.seed_', \n",
    "                      seed, '.', format(number_droplets, scientific=FALSE), '_droplets.iteration_',\n",
    "                      '#', '.npz', sep='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-configuration",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results <- function(phenopath_results, true_times) {\n",
    "    uncensored_results <- interaction_effects(phenopath_results)\n",
    "    censored_results <- significant_interactions(phenopath_results) * uncensored_results\n",
    "    pearson <- abs(cor(true_times, trajectory(phenopath_results))) # same level of support if pseudotimes are flipped\n",
    "    spearman <- abs(spearman(true_times, trajectory(phenopath_results)))\n",
    "    \n",
    "    results <- list(\"uncensored_results\" = uncensored_results, \n",
    "                   \"censored_results\" = censored_results,\n",
    "                   \"pearson\" = pearson,\n",
    "                   \"spearman\" = spearman)\n",
    "    return(results)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-amplifier",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_results_filename <- function(base_dir, scaling, iteration_number) {\n",
    "    iteration_filename = paste('iteration_', iteration_number, '.npz', sep='')\n",
    "    results_dir = file.path(paste(base_dir, '/', scaling, sep=''))\n",
    "    results_filename = file.path(paste(results_dir, '/', iteration_filename, sep=''))\n",
    "    return(results_filename)\n",
    "}\n",
    "\n",
    "save_results <- function(results_filename, results) {\n",
    "    np$savez_compressed(results_filename,\n",
    "    uncensored_results = results$uncensored_results,\n",
    "    censored_results = results$censored_results,\n",
    "    pearson = results$pearson,\n",
    "    spearman = results$spearman)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-consistency",
   "metadata": {},
   "source": [
    "The questions that I seek to answer are at a broad level: 'How can phenopath best be applied to this problem?' and 'What settings will allow phenopath to work best?'. Admittedly though not all possible choices of settings are being considered (e.g. alternative choices of `z_init`, like the true time values or the exponentials thereof, nor centering but not scaling the data, etc.) but the idea behind that was because in practice the experimenter is only likely to be willing (or in some cases only able) to use settings which do not deviate too much from the default values, plus it is reasonable to think that default choices of e.g. `z_init` were well thought out by the developers and are likely to be useful in practice, even if not the most useful theoretically possible for this context (which would be impossible to find, at least without a lot of probably intractable theory, due to e.g. the infinite state space of options for many settings).\n",
    "\n",
    "So the specific questions reduce to: (1) does it make sense to scale the data in this context (even though the data is PCR-bias adjusted and so roughly absolute counts), (2) what makes the most sense to use as the covariates for the model, and (3) do censoring values considered \"insignificant\" by the phenopath model lead to increased fit (i.e. by reducing false positives)? As an additional fun question, it is also asked: does the correlation (Pearson and/or Spearman) of the computed pseudotimes with the true times of the batches have any predictive value for the performance? E.g. do the true time values have any \"importance\" in that sense?\n",
    "\n",
    "As for always using the log-transformed data as the values, well that is both what is recommended in the phenopath paper and in its documentation, and moreover what makes the (generative) model for phenopath most closely resemble the (integral version of) the gLV equations.\n",
    "\n",
    "create directories to store results in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portable-blogger",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results_dir = 'phenopath_results_cells'\n",
    "if (!dir.exists(all_results_dir)) {dir.create(all_results_dir)}\n",
    "\n",
    "binary_results_dir = file.path(paste(all_results_dir, '/', 'binary_covariates', sep=''))\n",
    "if (!dir.exists(binary_results_dir)) {dir.create(binary_results_dir)}\n",
    "\n",
    "counts_cov_results_dir = file.path(paste(all_results_dir, '/', 'count_covariates', sep=''))\n",
    "if (!dir.exists(counts_cov_results_dir)) {dir.create(counts_cov_results_dir)}\n",
    "\n",
    "for (subdirectory in list.files(path=all_results_dir, full.names=T)) {\n",
    "    scaled_results_dir = file.path(paste(subdirectory, '/', 'scaled', sep=''))\n",
    "    if (!dir.exists(scaled_results_dir)) {dir.create(scaled_results_dir)}\n",
    "    \n",
    "    unscaled_results_dir = file.path(paste(subdirectory, '/', 'unscaled', sep=''))\n",
    "    if (!dir.exists(unscaled_results_dir)) {dir.create(unscaled_results_dir)}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-turkish",
   "metadata": {},
   "source": [
    "loop through the iterations of stored results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "needed-bedroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "# very bad naughty code that uses lots of global variables\n",
    "# this is a self-contained notebook however and not a software library.\n",
    "# I don't want to rewrite the function and the function calls to unnecessarily be\n",
    "# much longer when scope rules automatically do the right thing.\n",
    "lapply_input <- function(iteration_number){\n",
    "    \n",
    "    filename = gsub(\"#\", iteration_number, base_filename)\n",
    "    npzfile = np$load(filename)\n",
    "\n",
    "    cell_log_counts = npzfile[[\"cell_log_counts\"]]\n",
    "    cell_init_vectors = npzfile[[\"cell_init_vectors\"]]\n",
    "    cell_counts = exp(cell_log_counts)*(cell_log_counts != 0)    \n",
    "    \n",
    "    merged_droplets_per_batch <- dim(cell_log_counts)[1]/number_batches\n",
    "    true_times = c()\n",
    "    for (i in 1:number_batches) {true_times <- append(true_times, rep(i, merged_droplets_per_batch))}\n",
    "    \n",
    "    start_time <- proc.time()\n",
    "    binary_scaled <- phenopath_defaults(cell_log_counts, cell_init_vectors, scale_y=TRUE)\n",
    "    results <- get_results(binary_scaled, true_times)\n",
    "    save_results(get_results_filename(binary_results_dir, 'scaled', iteration_number), results)\n",
    "    run_time <- proc.time() - start_time; print(run_time)\n",
    "    \n",
    "    start_time <- proc.time()\n",
    "    binary_unscaled <- phenopath_defaults(cell_log_counts, cell_init_vectors, scale_y=FALSE)\n",
    "    results <- get_results(binary_unscaled, true_times)\n",
    "    save_results(get_results_filename(binary_results_dir, 'unscaled', iteration_number), results)\n",
    "    run_time <- proc.time() - start_time; print(run_time)\n",
    "    \n",
    "    start_time <- proc.time()\n",
    "    counts_scaled <- phenopath_defaults(cell_log_counts, cell_counts, scale_y=TRUE)\n",
    "    results <- get_results(counts_scaled, true_times)\n",
    "    save_results(get_results_filename(counts_cov_results_dir, 'scaled', iteration_number), results)\n",
    "    run_time <- proc.time() - start_time; print(run_time)\n",
    "    \n",
    "    start_time <- proc.time()\n",
    "    counts_unscaled <- phenopath_defaults(cell_log_counts, cell_counts, scale_y=FALSE)\n",
    "    results <- get_results(counts_unscaled, true_times)\n",
    "    save_results(get_results_filename(counts_cov_results_dir, 'unscaled', iteration_number), results)\n",
    "    run_time <- proc.time() - start_time; print(run_time)\n",
    "    \n",
    "    # this is supposed to be an embarrassingly parallel for loop, so memory usage should not change with number of iterations\n",
    "    # but system monitor shows memory usage continually increasing. Hadley Wickham seems to have said that calling `gc`\n",
    "    # manually for garbage collection should never be necessary, but honestly at this point I don't trust R so...\n",
    "    gc()\n",
    "    \n",
    "    # controversial stylistically but consistent with Python style and since most programming is done\n",
    "    # in Python, from a practical perspective it's better for me to use stylistic conventions that also work in Python.\n",
    "    return(NULL)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forced-awareness",
   "metadata": {},
   "outputs": [],
   "source": [
    "lapply(1:100, lapply_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.0.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
